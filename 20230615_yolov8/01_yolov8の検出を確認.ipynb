{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv8\n",
    "\n",
    "[https://github.com/ultralytics/ultralytics](https://github.com/ultralytics/ultralytics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (8.0.128)\n",
      "Requirement already satisfied: matplotlib>=3.2.2 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from ultralytics) (3.7.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from ultralytics) (4.8.0.74)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from ultralytics) (10.0.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from ultralytics) (6.0)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from ultralytics) (1.11.1)\n",
      "Requirement already satisfied: torch>=1.7.0 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from ultralytics) (2.0.1)\n",
      "Requirement already satisfied: torchvision>=0.8.1 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from ultralytics) (0.15.2)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from ultralytics) (4.65.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from ultralytics) (2.0.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from ultralytics) (0.12.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from ultralytics) (5.9.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics) (1.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics) (23.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from matplotlib>=3.2.2->ultralytics) (5.12.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2023.5.7)\n",
      "Requirement already satisfied: filelock in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from torch>=1.7.0->ultralytics) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from torch>=1.7.0->ultralytics) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from torch>=1.7.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from torch>=1.7.0->ultralytics) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from torch>=1.7.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib>=3.2.2->ultralytics) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from jinja2->torch>=1.7.0->ultralytics) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\hirahara\\anaconda3\\envs\\sentan\\lib\\site-packages (from sympy->torch>=1.7.0->ultralytics) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2, random, tempfile, requests\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "def show_img(img, dpi=150, title=None):\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, dpi=dpi)\n",
    "    ax.set_title( title, fontsize=16, color='black')\n",
    "    ax.axis('off')\n",
    "    ax.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    return fig, ax\n",
    "\n",
    "def show_imgs(imgs_dict:dict, ncol=0, dpi=200, font_scale=0.7):\n",
    "    font_size = int(plt.rcParams[\"font.size\"]*font_scale)\n",
    "\n",
    "    if ncol > 0:\n",
    "        nrow = ((len(imgs_dict)-1)//ncol)+1\n",
    "    else:\n",
    "        nrow = 1\n",
    "        ncol = len(imgs_dict)\n",
    "\n",
    "    img_num = len(imgs_dict)\n",
    "    fig = plt.figure(figsize=(float(img_num), float(img_num)), dpi=dpi)\n",
    "    grid = ImageGrid(fig, 111, nrows_ncols=(nrow, ncol), axes_pad=0.2,)\n",
    "\n",
    "    for i in range(nrow*ncol):\n",
    "        grid[i].axis('off')\n",
    "        if i < len(imgs_dict):\n",
    "            img_key = list(imgs_dict.keys())[i]\n",
    "            grid[i].imshow(cv2.cvtColor(imgs_dict[img_key], cv2.COLOR_BGR2RGB))\n",
    "            grid[i].set_title(img_key, fontsize=font_size, color='black', pad=int(font_size/2))\n",
    "    \n",
    "    plt.show(); plt.close()\n",
    "    return None\n",
    "\n",
    "def imread_web(url):\n",
    "    res = requests.get(url)\n",
    "    img = None\n",
    "    with tempfile.NamedTemporaryFile(dir=str(Path('./'))) as fp:\n",
    "        fp.write(res.content)\n",
    "        fp.file.seek(0)\n",
    "        img = cv2.imread(fp.name)\n",
    "    return img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 検出モデルを選ぶ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = [\"n\", \"s\", \"m\", \"l\", \"x\"][0]\n",
    "\n",
    "### 通常の物体検出モデル\n",
    "model = YOLO(f\"./models/yolov8{model_size}.pt\")\n",
    "\n",
    "# インスタンスセグメンテーションのモデル\n",
    "# model = YOLO(f\"./models/yolov8{model_size}-seg.pt\")\n",
    "\n",
    "### Keypoint検出（骨格検出）のモデル\n",
    "# model = YOLO(f\"./models/yolov8{model_size}-pose\")\n",
    "\n",
    "\n",
    "# 自作モデル\n",
    "# model = YOLO(f\"./__output_yolov8__/20230615-1346_yolov8n_nakane_grape1/weights/best.pt\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ローカルのサンプルデータで実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m imgs[\u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(\u001b[39mstr\u001b[39m(sample_imgPath))\n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(model\u001b[39m.\u001b[39mnames)\n\u001b[0;32m---> 13\u001b[0m result \u001b[39m=\u001b[39m model(\n\u001b[1;32m     14\u001b[0m                 source    \u001b[39m=\u001b[39;49m imgs[\u001b[39m\"\u001b[39;49m\u001b[39mraw\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     15\u001b[0m                 conf      \u001b[39m=\u001b[39;49m \u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     16\u001b[0m                 iou       \u001b[39m=\u001b[39;49m \u001b[39m0.001\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m                 save      \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     18\u001b[0m                 max_det   \u001b[39m=\u001b[39;49m \u001b[39m300\u001b[39;49m,\n\u001b[1;32m     19\u001b[0m                 augment   \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     20\u001b[0m                 classes   \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m, \u001b[39m# [1, 2, 3],\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m             )\n\u001b[1;32m     24\u001b[0m imgs[\u001b[39m\"\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m result[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mplot()\n\u001b[1;32m     26\u001b[0m show_imgs(imgs, dpi\u001b[39m=\u001b[39m\u001b[39m600\u001b[39m, font_scale\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/aiaug/lib/python3.9/site-packages/ultralytics/yolo/engine/model.py:111\u001b[0m, in \u001b[0;36mYOLO.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, source\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, stream\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    110\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calls the 'predict' function with given arguments to perform object detection.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(source, stream, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/aiaug/lib/python3.9/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/aiaug/lib/python3.9/site-packages/ultralytics/yolo/engine/model.py:250\u001b[0m, in \u001b[0;36mYOLO.predict\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask \u001b[39m=\u001b[39m overrides\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mtask\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask\n\u001b[1;32m    249\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor \u001b[39m=\u001b[39m TASK_MAP[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtask][\u001b[39m3\u001b[39m](overrides\u001b[39m=\u001b[39moverrides, _callbacks\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks)\n\u001b[0;32m--> 250\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredictor\u001b[39m.\u001b[39;49msetup_model(model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, verbose\u001b[39m=\u001b[39;49mis_cli)\n\u001b[1;32m    251\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# only update args if predictor is already setup\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m get_cfg(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredictor\u001b[39m.\u001b[39margs, overrides)\n",
      "File \u001b[0;32m~/anaconda3/envs/aiaug/lib/python3.9/site-packages/ultralytics/yolo/engine/predictor.py:303\u001b[0m, in \u001b[0;36mBasePredictor.setup_model\u001b[0;34m(self, model, verbose)\u001b[0m\n\u001b[1;32m    301\u001b[0m model \u001b[39m=\u001b[39m model \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mmodel\n\u001b[1;32m    302\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhalf \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m device\u001b[39m.\u001b[39mtype \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m  \u001b[39m# half precision only supported on CUDA\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m AutoBackend(model,\n\u001b[1;32m    304\u001b[0m                          device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m    305\u001b[0m                          dnn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mdnn,\n\u001b[1;32m    306\u001b[0m                          data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mdata,\n\u001b[1;32m    307\u001b[0m                          fp16\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mhalf,\n\u001b[1;32m    308\u001b[0m                          fuse\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    309\u001b[0m                          verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m    310\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m device\n\u001b[1;32m    311\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m~/anaconda3/envs/aiaug/lib/python3.9/site-packages/ultralytics/nn/autobackend.py:93\u001b[0m, in \u001b[0;36mAutoBackend.__init__\u001b[0;34m(self, weights, device, dnn, data, fp16, fuse, verbose)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39m# NOTE: special case: in-memory pytorch model\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m nn_module:\n\u001b[0;32m---> 93\u001b[0m     model \u001b[39m=\u001b[39m weights\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m     94\u001b[0m     model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfuse(verbose\u001b[39m=\u001b[39mverbose) \u001b[39mif\u001b[39;00m fuse \u001b[39melse\u001b[39;00m model\n\u001b[1;32m     95\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m'\u001b[39m\u001b[39mkpt_shape\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/anaconda3/envs/aiaug/lib/python3.9/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/envs/aiaug/lib/python3.9/site-packages/ultralytics/nn/tasks.py:181\u001b[0m, in \u001b[0;36mBaseModel._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    171\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m    `_apply()` is a function that applies a function to all the tensors in the model that are not\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[39m    parameters or registered buffers\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39m        A model that is a Detect() object.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    182\u001b[0m     m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]  \u001b[39m# Detect()\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(m, (Detect, Segment)):\n",
      "File \u001b[0;32m~/anaconda3/envs/aiaug/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/aiaug/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/aiaug/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/aiaug/lib/python3.9/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/aiaug/lib/python3.9/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "imgs = {}\n",
    "\n",
    "# img_dir = Path(\"./yolo_datasets/nakane_grape1/train/images/\")\n",
    "img_dir = Path(\"./sample_data/\")\n",
    "\n",
    "sample_imgPath = random.choice(list(img_dir.glob(\"*.jpg\")))\n",
    "\n",
    "img = cv2.imread(str(sample_imgPath))\n",
    "imgs[\"raw\"] = cv2.imread(str(sample_imgPath))\n",
    "\n",
    "# print(model.names)\n",
    "\n",
    "result = model(\n",
    "                source    = imgs[\"raw\"],\n",
    "                conf      = 0.1,\n",
    "                iou       = 0.001,\n",
    "                save      = False,\n",
    "                max_det   = 300,\n",
    "                augment   = True,\n",
    "                classes   = None, # [1, 2, 3],\n",
    "            )\n",
    "\n",
    "\n",
    "imgs[\"result\"] = result[0].plot()\n",
    "\n",
    "show_imgs(imgs, dpi=600, font_scale=0.2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ネット上の写真で実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING  'source' is missing. Using 'source=https://ultralytics.com/images/bus.jpg'.\n",
      "\n",
      "Found https:\\ultralytics.com\\images\\bus.jpg locally at bus.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image 1/1 c:\\Users\\hirahara\\Documents\\Python Scripts\\sentan2023\\20230615_yolov8\\bus.jpg: 640x480 2 persons, 1 bus, 1 stop sign, 249.1ms\n",
      "Speed: 2.4ms preprocess, 249.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 23\u001b[0m\n\u001b[0;32m     10\u001b[0m result \u001b[39m=\u001b[39m model(\n\u001b[0;32m     11\u001b[0m                 source    \u001b[39m=\u001b[39m imgs[\u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m     12\u001b[0m                 conf      \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     17\u001b[0m                 classes   \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m# [1, 2, 3],\u001b[39;00m\n\u001b[0;32m     18\u001b[0m             )\n\u001b[0;32m     21\u001b[0m imgs[\u001b[39m\"\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m result[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mplot()\n\u001b[1;32m---> 23\u001b[0m show_imgs(imgs, dpi\u001b[39m=\u001b[39;49m\u001b[39m600\u001b[39;49m, font_scale\u001b[39m=\u001b[39;49m\u001b[39m0.2\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[19], line 35\u001b[0m, in \u001b[0;36mshow_imgs\u001b[1;34m(imgs_dict, ncol, dpi, font_scale)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m \u001b[39mlen\u001b[39m(imgs_dict):\n\u001b[0;32m     34\u001b[0m         img_key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(imgs_dict\u001b[39m.\u001b[39mkeys())[i]\n\u001b[1;32m---> 35\u001b[0m         grid[i]\u001b[39m.\u001b[39mimshow(cv2\u001b[39m.\u001b[39;49mcvtColor(imgs_dict[img_key], cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2RGB))\n\u001b[0;32m     36\u001b[0m         grid[i]\u001b[39m.\u001b[39mset_title(img_key, fontsize\u001b[39m=\u001b[39mfont_size, color\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mblack\u001b[39m\u001b[39m'\u001b[39m, pad\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(font_size\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m))\n\u001b[0;32m     38\u001b[0m plt\u001b[39m.\u001b[39mshow(); plt\u001b[39m.\u001b[39mclose()\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgs = {}\n",
    "\n",
    "# sample_img_url = \"https://gahag.net/img/201607/05s/gahag-0103196580-1.jpg\"\n",
    "sample_img_url = \"https://picsum.photos/200/300\"\n",
    "imgs[\"raw\"] = imread_web(sample_img_url)\n",
    "print(imgs[\"raw\"])\n",
    "\n",
    "print(model.names)\n",
    "\n",
    "result = model(\n",
    "                source    = imgs[\"raw\"],\n",
    "                conf      = 0.1,\n",
    "                iou       = 0.001,\n",
    "                save      = False,\n",
    "                max_det   = 300,\n",
    "                augment   = True,\n",
    "                classes   = None, # [1, 2, 3],\n",
    "            )\n",
    "\n",
    "\n",
    "imgs[\"result\"] = result[0].plot()\n",
    "\n",
    "show_imgs(imgs, dpi=600, font_scale=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hirahara",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97a600b88c15305d6a6fa583345f30dfa16934507edfbb174ba624ca8bf61728"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
